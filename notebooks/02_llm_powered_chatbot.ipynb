{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build an LLM-Powered Chatbot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there, \n",
    "\n",
    "In this tutorial, we will be building an LLM-Powered chatbot. This chatbot will have the ability to have conversations and remember previous conversations.\n",
    "\n",
    "**NOTE:** This bot will only the language model to have conversations, we will cover more advanced topic later like:\n",
    "\n",
    "- **Conversation RAG:** Enable chatbot experience over external data source\n",
    "- **Agents:** Build a chatbot that can take actions\n",
    "\n",
    "Nevertheless, this tutorial will cover the basics required for those advance topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Jupyter Notebook**\n",
    "\n",
    "This tutorial uses Jupyter notebooks, Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through tutorials in an interactive environment is a great way to better understand them.\n",
    "\n",
    "\n",
    "This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following commands in your terminal to install the required packages (if you don't have them already):**\n",
    "\n",
    "- pip install langchain\n",
    "- pip install -qU langchain-groq\n",
    "- pip install langchain_community\n",
    "- pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quickstart**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by using the language model as is. We will be using llama3.1 from `groq` API. \n",
    "\n",
    "\n",
    "You can also run the llama3.1 model on your local system using `ollama`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To interact with a chatmodel, you simply pass a `HumanMessage` and call the `.invoke` method on the model**\n",
    "\n",
    "Let's seee how it is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Assalamu alaikum (or peace be with you!) Abdulmalik! Nice to meet you. What's on your mind, what can I help you with?\", response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 17, 'total_tokens': 54, 'completion_time': 0.049333333, 'prompt_time': 0.005726894, 'queue_time': None, 'total_time': 0.055060227}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-995f7c13-dbc4-4145-b948-3e1de99175b8-0', usage_metadata={'input_tokens': 17, 'output_tokens': 37, 'total_tokens': 54})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the HummanMessage function \n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "#Create a HumanMessage\n",
    "human_message = HumanMessage(content=\"Hi! I'm Abdulmalik\")\n",
    "\n",
    "#Insert the human message into the .invoke method\n",
    "model.invoke([human_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Will it remember my name?**\n",
    "\n",
    "Unfortunately, the model on it's own does not have any concept of memory, so no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm a large language model, I don't have the ability to know your personal information or remember previous conversations. Each time you interact with me, it's a new conversation and I don't retain any information about you.\\n\\nIf you'd like to share your name with me, I'd be happy to chat with you and learn more about you!\", response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 15, 'total_tokens': 86, 'completion_time': 0.094666667, 'prompt_time': 0.006263864, 'queue_time': None, 'total_time': 0.10093053099999999}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-2f22e047-af44-468e-80c5-c8f466758905-0', usage_metadata={'input_tokens': 15, 'output_tokens': 71, 'total_tokens': 86})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a HumanMessage\n",
    "human_message = HumanMessage(content=\"What is my name?\")\n",
    "\n",
    "#Insert the human message into the .invoke method\n",
    "model.invoke([human_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it doesn't take into consideration previous conversations.\n",
    "\n",
    "\n",
    "A chatbot that doesn't remember previous conversations will provide a terrible experience. **How do we fix this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get around this, we need to pass the entire `conversation history` into the model. \n",
    "\n",
    "\n",
    "Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Abdulmalik! How would you like me to address you? Would you prefer Abdulmalik, Abdul, or something else?', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 68, 'total_tokens': 98, 'completion_time': 0.04, 'prompt_time': 0.019601628, 'queue_time': None, 'total_time': 0.059601628000000004}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f66ccb39ec', 'finish_reason': 'stop', 'logprobs': None}, id='run-2c392c5c-459b-43aa-9e3c-ad5cdbbbf9ba-0', usage_metadata={'input_tokens': 68, 'output_tokens': 30, 'total_tokens': 98})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the AIMessage function from langchain, so we can mimick a response from the AI model \n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Abdulmalik\"),\n",
    "        AIMessage(content=\"Assalamu alaikum (or peace be with you!) Abdulmalik! Nice to meet you. What's on your mind, what can I help you with?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic idea underpinning a chatbot's ability to interact conversationally. \n",
    "\n",
    "\n",
    "**Now, how do we best implement this functionality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Message History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MessageHistory class would allow our model to remember previous conversations.\n",
    "\n",
    "\n",
    "- It will keep track of the inputs and outputs of the AI Model as store them in some datastore (memory).\n",
    "\n",
    "\n",
    "- Subsequent interactions will then load the those messages and pass them into the chain (combination of the AI model and other components) as part of the input. \n",
    "\n",
    "\n",
    "Let's see how to use this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important question to address first is, **How do we get the message history (previous messages)?**\n",
    "\n",
    "\n",
    "- To achieve this, we will create a function called `get_message_history`\n",
    "- This function is expected to take in a `session_id` and return a `Message History` object. \n",
    "- This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the `config` when calling the new chain (combination of the model and other components, like message history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see how it is done!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "#Create a storage\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_message_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Create the chain that has message history\n",
    "chain_with_message_history = RunnableWithMessageHistory(model, get_message_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now need to create a `config` that we pass into the chain every time.**\n",
    "\n",
    "- This config contains information that is not part of the input directly, but is still useful. \n",
    "- In this case, we want to include a `session_id`. \n",
    "\n",
    "\n",
    "This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Assalamu alaikum, Abdulmalik! Nice to meet you. How are you today? What brings you here? Do you want to talk about something specific or just say hello? I'm here to listen!\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the config \n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "\n",
    "# Create a variable called `ai_response` to store the output of the AI model\n",
    "ai_response = chain_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Abdulmalik\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Show the content of the AI response\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Welcome! You seem a bit... stuck. No worries! How about this: would you like to break the ice by sharing what brings you to our chat, or do you need a random fun question to get the conversation going? For example: What's your favorite hobby? Or where are you from? Or anything at all? I'm all ears (well, not really, but I'll still listen with my text self!)\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the config \n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "\n",
    "# Create a variable called `ai_response` to store the output of the AI model\n",
    "ai_response = chain_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Abdulmalik\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Show the content of the AI response\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I think I see where this is going! Your name is Abdulmalik! I remember from earlier, you told me that you're Abdulmalik! Don't worry, I won't hold it against you if you forgot â€“ we all forget sometimes! Do you want to try again, or move on to something new?\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the config \n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "\n",
    "# Create a variable called `ai_response` to store the output of the AI model\n",
    "ai_response = chain_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! WHat is my name\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Show the content of the AI response\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great! Our chatbot now remembers things about us.**\n",
    "\n",
    "- If we change the config to reference a different `session_id`, we can see that it starts the conversation fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, I'm a large language model, I don't have any prior information about you, so I don't know your name yet! Would you like to introduce yourself? I'd love to get to know you!\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the config \n",
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "\n",
    "# Create a variable called `ai_response` to store the output of the AI model\n",
    "ai_response = chain_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! WHat is my name\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Show the content of the AI response\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can always go back to the previous conversation by passing the right session id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You're asking again! Your name is... Abdulmalik!\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the config \n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "\n",
    "# Create a variable called `ai_response` to store the output of the AI model\n",
    "ai_response = chain_with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! WHat is my name\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Show the content of the AI response\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is how we can support a chatbot having conversations with many users!**\n",
    "\n",
    "\n",
    "**Now, we can start to make the chatbot more complicated and personalized by adding in a prompt template.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompt Template**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To put is simply, Prompt Templates gives you the flexibility to control the behaviour of your AI model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by adding a system message th some custom instructions for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We will use a `MessagePlaceholder` to pass in all input messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple chain with the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required functions\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template and save it in a variable called \"prompt\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful and polite assistant that provides excellent customer service experience to customers. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now invoke the chain and pass in a language of our choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Abdulmalik! It's nice to meet you. Welcome to our service. How can I assist you today? Are you looking for information, need help with something, or perhaps have a question? I'm all ears and here to help.\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm Abdulmalik\")], \"language\": \"English\"}\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now wrap this more complicated chain in a Memory  (Message History class).**\n",
    "\n",
    "- This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Abdulmalik! It's nice to meet you! How can I assist you today? Are you looking for information on a particular topic, or do you have a question about something? I'm all ears and here to help!\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abcd1\"}}\n",
    "\n",
    "\n",
    "\n",
    "ai_response = chain_with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm Abdulmalik\")], \"language\": \"English\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Abdulmalik. Is there something specific you'd like to know about your name, or is there anything else I can help you with?\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chain_with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")], \"language\": \"English\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's be a bit rude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I apologize if my previous response did not meet your expectations. I'm committed to providing a high-quality service and respecting all customers. I'll do my best to turn things around and provide a better experience for you.\\n\\nTo start fresh, let me try again. Your name is Abdulmalik, and I'm here to assist you with any questions or concerns you may have. Can you please tell me more about what's not meeting your expectations and how I can improve? I'm listening and eager to help.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chain_with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"I think you are being unprofessional and your service is trash.\")], \"language\": \"English\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Managing Conversation History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "**Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.**\n",
    "\n",
    "- We can do this by adding a simple step in front of the prompt that modifies the `messages` key appropriately, \n",
    "- and then wrap that new chain in the Message History class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use the `trim_messages` helper function to reduce how many messages we're sending to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm Abdulmalik\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='I like vanilla ice cream'),\n",
       " AIMessage(content='nice'),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `trimmer` in our chain, we just need to run it before we pass the messages input to our prompt.\n",
    "\n",
    "\n",
    "Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't think you mentioned your name earlier. You're just a customer chatting with me, so I don't have any information about your name. Would you like to introduce yourself?\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ai_response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But if we ask about information that is within the last few messages, it remembers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked for the answer to the math problem 2 + 2.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Let's now wrap this in the Message History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm afraid I don't know your name. This is the beginning of our conversation, and I don't have any information about you yet. Would you like to introduce yourself?\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_response = chain_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "ai_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the first message where we stated our name has been trimmed. Plus there's now two new messages in the chat history (our latest question and the latest response). \n",
    "\n",
    "\n",
    "This means that even more information that used to be accessible in our conversation history is no longer available! \n",
    "\n",
    "\n",
    "In this case our initial math question has been trimmed from the history as well, so the model no longer knows about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You didn't ask a math problem. This is the beginning of our conversation, and I'm here to help with any questions or topics you'd like to discuss. How can I assist you today?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain_with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Streaming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a functional chatbot. \n",
    "\n",
    "However, one really important UX consideration for chatbot application is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most application do is stream back each token (word) as it is generated. \n",
    "\n",
    "This allows the user to see progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All chains expose a `.stream` method, and ones that use message history are no different. \n",
    "\n",
    "\n",
    "We can simply use that method to get back a streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Abdulmalik! It's great to meet you! Here's a joke for you:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "I hope that made you smile! Do you want to hear another one?"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "\n",
    "\n",
    "for response in chain_with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm Abdulmalik. tell me a joke\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    \n",
    "    print(response.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Steps**\n",
    "\n",
    "\n",
    "Now that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:\n",
    "\n",
    "\n",
    "- **Conversational RAG:** Enable a chatbot experience over an external source of data\n",
    "- **Agents:** Build a chatbot that can take actions\n",
    "\n",
    "\n",
    "**Stay Tuned!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
